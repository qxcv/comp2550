\documentclass[a4paper]{article}
\usepackage{
    hyperref,xcolor,todonotes,tabularx,multirow,multicol,titling,grffile,
    amsmath
}
\hypersetup{%
    % See
    % http://tex.stackexchange.com/questions/823/remove-ugly-borders-around-clickable-cross-references-and-hyperlinks
    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
}
\usepackage[round]{natbib}
% TODO: Reinstate this once I've got a high enough word count to reach 10 pages
% with normal margins.
% \usepackage[margin=1in]{geometry}

% Report plan:
%
% 1) Write a brief introduction outlining what I'm trying to do and why it's
% useful. Can just reword this from project proposal and slides. Maybe 0.5-1
% pages.
% 2) Talk about previous work. This will hopefully take up 1-1.5 pages.
% 3) Discuss the particle filter. I can make this as long as I want, but I
% would like it if I could spend about 1.5 pages talking about the PF.
% 4) Now I can introduce what I actually did. Probably start with the basic
% (map-free) structure, then move on to producing maps from OSM sources, then
% move on to incorporating this map data, then move onto the hacks I needed to
% complete to get the localiser to actually localise properly. This should take
% up the bulk of the report (2-3 pages)
% 5) Experiments. This will include a table for each of the KITTI datasets
% showing mean and standard deviation of HPE with and without the map enabled
% for particle filtering, and at various levels of noise. Should also show one
% or more qualitative experiments showing performance with *real* GPS data
% (ideally without IMU). Should also talk about the defects of the algorithm
% and how these could be fixed (e.g. with additional sensor data). This will
% hopefully take up another 2-3 pages.
% 6) Conclusion. This will be maybe 0.25 pages.
% 7) References. Don't think these count towards the total page sum.
%
% So, adding those up, I get a minimum of 0.5+1+2+2+0.25 = 5.75 pages and a
% maximum of 1+1.5+3+3+0.25 = 9.75 pages. Ugh, I'm going to have to be REALLY
% verbose and include a LOT of diagrams (that's how other authors pad out their
% papers, apparently).

\newcommand{\mat}{\mathbf}
\renewcommand{\b}{\mathbf}

\begin{document}
    \title{Robust Map-Augmented Localisation Using Particle Filters\\
    {\Large COMP2550 Project Report}}
    \author{Sam Toyer\\\texttt{u5568237@anu.edu.au}}
    \date{\today}

    \maketitle

    % TODO: Rewrite this once I figure out what the hell my report is actually
    % about.
    \abstract{%
        In this paper, we propose a particle filtering method which fuses GPS
        fixes, odometry data, and the information in a digital street map to
        localise a road vehicle with superior accuracy to methods based on GPS
        and odometry alone. We demonstrate that our algorithm is robust to GPS
        dropouts, and give a variant of the algorithm which dispenses with the
        requirement for odometry.
    }

    \section{Introduction}

    % TODO: Might want to use localisation of visually impaired users as another
    % example (which I did in my proposal).

    Localisation is a critical task for intelligent transport systems, and
    features in systems ranging from personal navigation devices
    \citep{white2000some} to self-driving cars \citep{levinson2011towards}.
    Accurate localisation data can also be used to augment vehicle-based systems
    not directly related to navigation. For example, this work was motivated in
    part by the visual road detection algorithm presented by
    \citet{alvarez2014combining}, which projects road segments stored in a map
    database into the field of view of a camera in order determine which pixels
    produced by the camera are most likely to correspond to a road. Precise
    information about the location of the vehicle is clearly of the utmost
    importance in such systems.

    This task is complicated by the noise present in sensor readings. Despite
    its usefulness, GPS is a major offender in this regard: GPS accuracy can
    range from around 2m in an open area to up to 15m in heavily built-up areas
    \citep{modsching2006field}. Dead reckoning also suffers from significant
    errors, although unlike GPS error these tend to gradually increase over
    time. For instance, \citet{vlcek1993gps} report that low cost dead reckoning
    systems---typically based on odometry and gyroscope data---yield errors of
    between 2\% and 5\% of the distance travelled.

%     TODO: Is it okay to do this? It's not plagiarism, but it may violate
%     copyright. Should ask Steve or someone else who's likely to know. Maybe ANU
%     has a specific policy here. If not, I'll just leave the image out, since
%     it's not critical to the report.
%     \begin{figure}
%         \centering
%         \includegraphics[width=0.45\textwidth]{images/drawil-et-al.png}
%         \caption{Figure from \citet{drawil2013gps} demonstrating the GPS fixes
%         produced by a receiver in a vehicle as the vehicle drove through a built
%         up area. Grey lines indicate roads, whilst coloured dots indicate GPS
%         fixes, coloured according to the output of an automated precision
%         classifier.}
%         \label{fig:drawil-et-al}
%     \end{figure}

    Fortunately, road vehicles in motion are almost always situated on public
    roads, so it is possible to improve the accuracy of a localisation system by
    taking into account the information about the public road network which is
    present in a digital street map. With the advent of the OpenStreetMap
    project\footnotemark---which provides, free of charge, detailed road network
    maps covering most of the globe---use of map information in this way is
    quickly becoming a cheap and convenient method of augmenting the accuracy of
    existing localisation systems.

    \footnotetext{\url{http://www.openstreetmap.org/}}

    The most common method of incorporating this information is through ``map
    matching'', where localisation is performed under the simplifying assumption
    that the true location will \emph{always} coincide with a roadway, footpath
    or other mapped feature. In this paper, we will not make such a highly
    restrictive assumption. Instead, we will merely assume that the vehicle is
    \emph{most likely} near a roadway, and attempt to use this assumption to
    improve localisation accuracy as greatly as we can without losing the
    ability to localise vehicles which stray from the road network.

    \section{Related work}

    The approaches which presently dominate the map-assisted localisation
    literature are map matching methods which use sets of hand-crafted rules to
    determine which road segment in a digital map a vehicle is most likely to be
    travelling on, then attempt to localise the vehicle within that road
    segment. Typically, these approaches work by first assigning a ``score'' to
    each road segment in a digital street map based on the segment's distance
    from the vehicle's estimated position, the segment's heading, and so on, and
    then localise to the point on the highest scoring segment which is nearest
    to the last GPS fix or Kalman filter state estimate
    \citep{quddus2007current}.

    These heuristic methods can achieve very high accuracy. For example,
    \citet{quddus2006high} reported that the road segment scoring portion of
    their algorithm was able to correctly identify 99.2\% of the 4,605 road
    segments in one of their data sets. However, such high accuracy is not
    representative of all heuristic map matching
    algorithms---\citeauthor{quddus2006high} reported previous algorithms
    achieving 70\%-98\% segment identification accuracy where theirs achieved
    99.2\%---and usually requires large sets of hand-tuned rules. Moving from a
    rural to an urban environment, for instance, can sometimes reduce the
    usefulness of these rules, necessitating the production of yet more rules
    and heuristics for different environments \citep{velaga2012improving}.
    Further, heuristic map-matching algorithms are highly sensitive to initial
    conditions, as producing an incorrect first match will likely cause
    subsequent matches to be incorrect due to the fact that heuristic algorithms
    often assign low scores to road segments far away from previously matched
    segments \citep{syed2004fuzzy}.

    Probabilistic approaches to map aided localisation manage to avoid some of
    the pitfalls of heuristic approaches. Rather than producing a single ``best
    guess'' of the vehicle's state at each time step, probabilistic methods
    internally maintain a probability distribution over all possible vehicle
    states. This distribution may be updated sequentially, and used to produce
    the expectation of the vehicle's position at each time step. This approach
    makes it straightforward to incorporate almost any kind of data: for
    instance, \citet{brubaker2013lost} used a probabilistic approach based on
    Gaussian mixture models to localise a vehicle using only street map
    information and visual odometry---a feat which would be impossible with
    traditional heuristic map matching approaches. Probabilistic approaches also
    make it possible to relax the map matching assumption that the vehicle is
    always on a mapped feature, as it is straightforward to expand  the modelled
    distribution over possible vehicle states to cover both on-road and off-road
    areas.

    Particle filtering---also known as Monte Carlo localisation---is a popular
    probabilistic localisation method which has been employed for map aided
    localisation in a number of different ways. \citet{chausse2005vehicle}
    applied a particle filter to fuse odometry, GPS, vision and the information
    in a digital map, whilst \citet{selloum2009lane} and
    \citet{toledo2009fusing} used particle filtering with odometry, GPS and map
    information alone. In each instance, the authors reported excellent results,
    with all algorithms providing lane-level positioning accuracy in some cases.

    Unfortunately, the aforementioned approaches to probabilistic map-aided
    localisation both required sensors and information which are not always
    available. In all three cases, the algorithms required extremely precise
    maps offering a level of detail far beyond what could be expected of an
    ordinary digital street map used for navigation. Furthermore,
    \citeauthor{selloum2009lane} and \citeauthor{toledo2009fusing} both assumed
    the availability of a service like the European Geostationary Navigation
    Overlay Service (EGNOS) to correct GPS errors, whilst
    \citeauthor{chausse2005vehicle} required that the vehicle be fitted with a
    camera in order to achieve lane-level accuracy. These requirements are all
    difficult to satisfy for low-cost systems and systems which operate in
    remote regions, so one of our aims in this paper will be to dispense with
    these onerous requirements.

    \section{Our approach}\label{sec:algorithm}
    \subsection{Particle filtering}

    As mentioned previously, particle filtering is a probabilistic localisation
    method that is well suited to map-aided localisation. Internally, the
    particle filter represents a distribution over possible vehicle states with
    a fixed number of ``particles'', each of which represents a single sample
    from the following distribution \citep{fox1999monte}:

    \[
        p(s_t \mid o_t, a_{t-1}, o_{t-1}, a_{t-2}, \ldots, o_0)
    \]

    Where:

    \begin{description}
    \item[$s_t$] is the vehicle state at time $t$. This generally consists of a
    latitude, longitude and heading, but may contain more information in some
    cases.
    \item[$o_t$] represents the set of observations made by the vehicle's
    sensors at time $t$.
    \item[$a_t$] is the ``action'' taken by the vehicle at time $t$. Actions do
    not necessarily have to correspond to the output of a specific actuator; in
    our case, we will consider a vehicle's actions to be the distances reported
    by its wheel speed sensors. Although these are technically sensory
    observations, it is convenient to treat them separately to the other
    observations $o_t$ for the purposes of particle filtering.
    \end{description}

    \subsection{Incorporating map data}

    Whilst other authors have attempted to incorporate street map data into the
    particle filtering process by extending each particle's state to include an
    associated road segment \citep{selloum2009lane,toledo2009fusing}, we have
    found that the fastest way of incorporating map data is by way of a
    pseudo-likelihood during the particle filter's update step. Concretely,
    for each particle $n$, we calculate the distance $d_n$ to the road segment
    in the stored digital street map, then update the weight $w^{(n)}$ of the
    stored particle using:

    % TODO: Update this when I'm finished. I'll probably choose a completely
    % different pseudolikelihood before I'm done, since this one seems
    % excessively restrictive (and leads to numerical issues).
    \[
        w^{(n)} \leftarrow \frac{w^{(n)}}{\left(1 + d_n^2\right)^{1.1}}
    \]

    We have found that this pseudo-likelihood does a reasonable job of
    forcing particles to remain near the road without preventing the particles
    from ``drifting'' away when GPS fixes are a long way from the map. This
    ensures that the vehicle is still able to localise itself when it is
    travelling on unmapped roads.

    % TODO: Can probably motivate this a little more by referring to SIR. Also,
    % I really need to look into ignoring map updates when the map is a suitable
    % distance (100m? 500m?) from the nearest particle to the map.

    % Efficiently incorporating map data
    Despite being an efficient option amongst different methods of incorporating
    map data, computing the distance between each particle and its nearest road
    segment is still by far the most expensive step in the particle filtering
    process, so it is imperative that this computation be fast if real-time
    performance is desired. We have found that the k-d tree implementation
    provided by \citet{alliez20153d} yields good performance for this task,
    taking around 50ms to perform the calculations for 2000 particles on a map
    of 3220 segments using a 2.5GHz Intel Core i5 processor.

    \subsection{GPS dropouts}

    \subsection{Unavailability of odometry}

    % TODO: Explain Markov chains in particle filter introduction
    If odometry is unavailable, the transition model proposed previously will
    cease to work, so we must produce a new transition model which does not
    depend on odometry.

    One alternative model would be to adopt a stationary transition
    distribution, like a zero-mean Gaussian. Such a transition model would be
    efficient and simple, but would violate the assumption that vehicle states
    form a Markov chain, since clearly a vehicle which moves by a certain amount
    and in a certain direction at one time step is most likely to move by a
    similar amount and in a similar direction at the next. Concretely, violating
    this assumption means that particles either move too little or too much at
    each time step, depending on the vehicle's velocity.

    In theory, the Markov state transition assumption can only be restored by
    expanding the internal state representation to contain all relevant physical
    properties of the vehicle, including position, velocity, acceleration,
    heading, angular velocity, angular acceleration, and so on. This is, of
    course, impossible, so we have chosen to expand the state representation to
    contain only velocity and position. This does not truly satisfy the Markov
    state transition assumption, since the changes in a vehicle's velocity are
    heavily correlated over time, but we have nonetheless found that it produces
    acceptable results.

    Formally, operating without odometry requires that we replace our previous
    state vector $s = \left(x, y, \theta\right)^T$ with a new state vector $s =
    \left(x, y, v_x, v_y\right)^T$, where $v_x$ and $v_y$ are easting and
    northing velocities, respectively. Given state $s_t$, our transition model
    becomes:

    \[
        s_{t+1} = \begin{bmatrix}
        1 & 0 & \Delta t & 0\\
        0 & 1 & 0 & \Delta t\\
        0 & 0 & 1 & 0\\
        0 & 0 & 0 & 1
        \end{bmatrix} s_t + \begin{bmatrix}
        0\\
        0\\
        \epsilon_x\\
        \epsilon_y
        \end{bmatrix}
    \]

    % TODO: OMG I CAN DO AN AWESOME PLOT OF VELOCITY TRANSITION PROBABILITIES,
    % THEN FIT A COVARIANCE TO IT!
    Where $(\epsilon_x, \epsilon_y) \sim \mathcal N(\b 0, \sigma^2 \b I)$ and
    $\Delta_t$ is the time elapsed between the $s_t$ and $s_{t+1}$. We have
    found that $\sigma^2 = \Delta t$ works well, although in general it should
    be possible to fit an appropriate Gaussian from training data.

    \section{Results}

    In order to evaluate the performance the map-based localisation algorithm
    proposed in Section~\ref{sec:algorithm}, we utilised both quantitative and
    qualitative assessment. In the quantitative tests, we attempted to analyse
    the performance of the algorithm on partially artificial data for which it
    was straightforward to measure divergence from the ground truth. On the
    other hand, the qualitative tests attempted to examine algorithm performance
    using more realistic consumer-grade localisation data, which unfortunately
    did not include a ground truth for use in numerical comparisons.

    The quantitative tests utilised the data set presented by
    \citet{brubaker2013lost}, which includes a number of GPS traces along with
    OpenStreetMap data for the regions immediately surrounding the traces. The
    supplied dataset included exceptionally accurate RTK-GPS positioning data,
    but no corresponding samples with a more representative noise model. As a
    result, we had to use a synthetic noise model to benchmark the algorithm,
    which included the following ``roughening'' steps:

    % TODO: Update this section with the parameters I *actually use* for the
    % final comparison.
    \begin{enumerate}
        \item Downsampling of GPS-like positioning information from 10Hz to 1Hz.
        \item Addition of isotropic Gaussian-distributed white noise with zero
        mean and variance $64$$\mathrm m^2$ to the positioning information.
        \item Rescaling of reported speed by a small but random factor of at
        most 1\% in order to emulate a subtly miscalibrated odometer.
        \item Addition of Gaussian noise with zero mean and standard deviation
        $4 \times 10^{-4}$ deg/$\mathrm s^2$ to the angular acceleration data.
    \end{enumerate}

    These parameters do not closely reflect the characteristics of real sensor
    noise. In particular, real GPS error tends to be highly time-correlated, but
    with an unpredictable distribution governed by atmospheric interference,
    multipath fading, and so on \citep{bajaj2002gps}. Nonetheless, this
    ``roughening'' process does an excellent job of illustrating the difference
    in performance between plain particle filtering and map-aided particle
    filtering in the presence of non-ideal sensory input.

    % TODO: Actually do the comparison and write up a table here

    The qualitative assessment used a GPS trace kindly provided by J. Alvarez
    (personal communication, May 12, 2015), which was produced from several
    kilometers of urban driving, including a brief GPS outage corresponding to a
    tunnel. The results of this test are presented in Figure~\ref{fig:qual}

    \begin{figure*}
    % See
    % https://tex.stackexchange.com/questions/122786/align-16-small-images-in-a-4-x-4-grid
    \centering
    \setlength{\tabcolsep}{0pt}
    % TODO: Good idea to timestamp these :-)
    \begin{tabular}{cccc}
    \includegraphics[width=0.25\textwidth]{spliced/with-map.avi-9.2.png}
    &
    \includegraphics[width=0.25\textwidth]{spliced/with-map.avi-12.4.png}
    &
    \includegraphics[width=0.25\textwidth]{spliced/with-map.avi-14.4.png}
    &
    \includegraphics[width=0.25\textwidth]{spliced/with-map.avi-16.8.png}
    \\
    \includegraphics[width=0.25\textwidth]{spliced/without-map.avi-9.2.png}
    &
    \includegraphics[width=0.25\textwidth]{spliced/without-map.avi-12.4.png}
    &
    \includegraphics[width=0.25\textwidth]{spliced/without-map.avi-14.4.png}
    &
    \includegraphics[width=0.25\textwidth]{spliced/without-map.avi-16.8.png}
    \end{tabular}
    \caption{Qualitative comparison of the map-aided particle filter (top) with
    an equivalent particle filter not making use of map information (bottom).
    Time flows from left to right. The most recent GPS fix in each frame is
    shown as a red cross, whilst the particle filter's state estimate is shown
    as a blue triangle and its particles are shown as red circles, with the area
    of each circle proportional to the corresponding particle's weight. The
    network of blue lines represent line centrelines.}
    \label{fig:qual}
    \end{figure*}

    % IDEA: Apparently along-track error is much lower than cross-track error in
    % built-up areas because of the fact that buildings, which cause multipath
    % interference, are located on either side of the road. This makes the fact
    % that my algorithm reduces along-track error quite helpful.

    \section{Conclusion}

    \bibliography{citations}{}
    \bibliographystyle{abbrvnat}
\end{document}

