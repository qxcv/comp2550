\documentclass[11pt]{article}

\usepackage{hyperref,xcolor,todonotes,tabularx,multirow,multicol,titling}

\hypersetup{
    % See
    % http://tex.stackexchange.com/questions/823/remove-ugly-borders-around-clickable-cross-references-and-hyperlinks
    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
}

\usepackage[round]{natbib}

\usepackage[margin=1in]{geometry}

% Report plan:
%
% 1) Write a brief introduction outlining what I'm trying to do and why it's
% useful. Can just reword this from project proposal and slides. Maybe 0.5-1
% pages.
% 2) Talk about previous work. This will hopefully take up 1-1.5 pages.
% 3) Discuss the particle filter. I can make this as long as I want, but I
% would like it if I could spend about 1.5 pages talking about the PF.
% 4) Now I can introduce what I actually did. Probably start with the basic
% (map-free) structure, then move on to producing maps from OSM sources, then
% move on to incorporating this map data, then move onto the hacks I needed to
% complete to get the localiser to actually localise properly. This should take
% up the bulk of the report (2-3 pages)
% 5) Experiments. This will include a table for each of the KITTI datasets
% showing mean and standard deviation of HPE with and without the map enabled
% for particle filtering, and at various levels of noise. Should also show one
% or more qualitative experiments showing performance with *real* GPS data
% (ideally without IMU). Should also talk about the defects of the algorithm
% and how these could be fixed (e.g. with additional sensor data). This will
% hopefully take up another 2-3 pages.
% 6) Conclusion. This will be maybe 0.25 pages.
% 7) References. Don't think these count towards the total page sum.
%
% So, adding those up, I get a minimum of 0.5+1+2+2+0.25 = 5.75 pages and a
% maximum of 1+1.5+3+3+0.25 = 9.75 pages. Ugh, I'm going to have to be REALLY
% verbose and include a LOT of diagrams (that's how other authors pad out their
% papers, apparently).

\begin{document}
    \title{Robust Map-Augmented Localisation Using Particle Filters\\
    {\Large COMP2550 Project Report}}
    \author{Sam Toyer\\\texttt{u5568237@anu.edu.au}}
    \date{\today}

    \maketitle

    % TODO: Rewrite this once I figure out what the hell my report is actually
    % about.
    \abstract{
        In this paper, we propose a particle filtering method which fuses GPS
        fixes, odometry data, and the information in a digital street map to
        localise a road vehicle with superior accuracy to methods based on GPS
        and odometry alone. We demonstrate that our algorithm is robust to GPS
        dropouts, and give a variant of the algorithm which dispenses with the
        requirement for odometry.
    }

    \section{Introduction}

    \section{Related work}

    % TODO: Explain heuristic algorithms, then segue into probabilistic
    % algorithms. Can really try to use up four or five pages this way.

    % Also need to talk, in particular, about Selloum et al.'s, article, as well
    % as Dellaert's article and the MCL papers.

    \section{Particle filtering}

    \section{The localisation algorithm}
    \label{sec:algorithm}

    \subsection{Incorporating map data}
    % Efficiently incorporating map data
    Computing the distance between each particle and its nearest road segment is
    by far the most expensive step in the particle filtering process, so it is
    imperative that this computation be fast if real-time performance is
    desired. We have found that the k-d tree implementation provided by
    \citet{alliez20153d} yields good performance for this task, taking around
    50ms to perform the calculations for 2000 particles on a map of 3220
    segments using a 2.5GHz Intel Core i5 processor.

    \subsection{GPS dropouts}

    \subsection{Unavailability of odometry}

    \section{Results}

    In order to evaluate the performance the map-based localisation algorithm
    proposed in Section~\ref{sec:algorithm}, we utilised both quantitative and
    qualitative assessment. In the quantitative tests, we attempted to analyse
    the performance of the algorithm on partially artificial data for which it
    was straightforward to measure divergence from the ground truth. On the
    other hand, the qualitative tests attempted to examine algorithm performance
    using more realistic consumer-grade localisation data, which unfortunately
    did not include a ground truth for use in numerical comparisons.

    The quantitative tests utilised the data set presented by
    \citet{brubaker2013lost}, which includes a number of GPS traces along with
    OpenStreetMap data for the regions immediately surrounding the traces. The
    supplied dataset included exceptionally accurate RTK-GPS positioning data,
    but no corresponding samples with a more representative noise model. As a
    result, we had to use a synthetic noise model to benchmark the algorithm,
    which included the following ``roughening'' steps:

    \begin{enumerate}
        % TODO
        \item
    \end{enumerate}

    The qualitative assessment used a GPS trace kindly provided by J. Alvarez
    (personal communication, May 12, 2015), which was produced from several
    kilometers of urban driving, including a brief GPS outage corresponding to a
    tunnel. The results of this test are presented in Figure % TODO

    \section{Conclusion}

    \bibliography{citations}{}
    \bibliographystyle{abbrvnat}
\end{document}
