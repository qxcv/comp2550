\documentclass[a4paper]{article}
\usepackage{hyperref,xcolor,todonotes,tabularx,multirow,multicol,titling}
\hypersetup{%
    % See
    % http://tex.stackexchange.com/questions/823/remove-ugly-borders-around-clickable-cross-references-and-hyperlinks
    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
}
\usepackage[round]{natbib}
\usepackage[margin=1in]{geometry}

% Report plan:
%
% 1) Write a brief introduction outlining what I'm trying to do and why it's
% useful. Can just reword this from project proposal and slides. Maybe 0.5-1
% pages.
% 2) Talk about previous work. This will hopefully take up 1-1.5 pages.
% 3) Discuss the particle filter. I can make this as long as I want, but I
% would like it if I could spend about 1.5 pages talking about the PF.
% 4) Now I can introduce what I actually did. Probably start with the basic
% (map-free) structure, then move on to producing maps from OSM sources, then
% move on to incorporating this map data, then move onto the hacks I needed to
% complete to get the localiser to actually localise properly. This should take
% up the bulk of the report (2-3 pages)
% 5) Experiments. This will include a table for each of the KITTI datasets
% showing mean and standard deviation of HPE with and without the map enabled
% for particle filtering, and at various levels of noise. Should also show one
% or more qualitative experiments showing performance with *real* GPS data
% (ideally without IMU). Should also talk about the defects of the algorithm
% and how these could be fixed (e.g. with additional sensor data). This will
% hopefully take up another 2-3 pages.
% 6) Conclusion. This will be maybe 0.25 pages.
% 7) References. Don't think these count towards the total page sum.
%
% So, adding those up, I get a minimum of 0.5+1+2+2+0.25 = 5.75 pages and a
% maximum of 1+1.5+3+3+0.25 = 9.75 pages. Ugh, I'm going to have to be REALLY
% verbose and include a LOT of diagrams (that's how other authors pad out their
% papers, apparently).

\begin{document}
    \title{Robust Map-Augmented Localisation Using Particle Filters\\
    {\Large COMP2550 Project Report}}
    \author{Sam Toyer\\\texttt{u5568237@anu.edu.au}}
    \date{\today}

    \maketitle

    % TODO: Rewrite this once I figure out what the hell my report is actually
    % about.
    \abstract{%
        In this paper, we propose a particle filtering method which fuses GPS
        fixes, odometry data, and the information in a digital street map to
        localise a road vehicle with superior accuracy to methods based on GPS
        and odometry alone. We demonstrate that our algorithm is robust to GPS
        dropouts, and give a variant of the algorithm which dispenses with the
        requirement for odometry.
    }

    \section{Introduction}

    Localisation is a critical task for intelligent transport systems, and
    features in systems ranging from personal navigation devices
    \citep{white2000some} to self-driving cars \citep{levinson2011towards}.
    Accurate localisation data can also be used to augment vehicle-based systems
    not directly related to navigation. For example, this work was motivated in
    part by the visual road detection algorithm presented by
    \citet{alvarez2014combining}, which projects road segments stored in a map
    database into the field of view of a camera in order determine which pixels
    produced by the camera are most likely to correspond to a road. Precise
    information about the location of the vehicle is clearly of the utmost
    importance in such systems.

    This task is complicated by the noise present in sensor readings. Despite
    its usefulness, GPS is a major offender in this regard: GPS accuracy can
    range from around 2m in an open area to up to 15m in heavily built-up areas
    \citep{modsching2006field}. Dead reckoning also suffers from significant
    errors, although unlike GPS error these tend to gradually increase over
    time. For instance, \citet{vlcek1993gps} report that low cost dead reckoning
    systems---typically based on odometry and gyroscope data---yield errors of
    between 2\% and 5\% of the distance travelled.

%     TODO: Is it okay to do this? It's not plagiarism, but it may violate
%     copyright. Should ask Steve or someone else who's likely to know. Maybe ANU
%     has a specific policy here. If not, I'll just leave the image out, since
%     it's not critical to the report.
%     \begin{figure}
%         \centering
%         \includegraphics[width=0.45\textwidth]{images/drawil-et-al.png}
%         \caption{Figure from \citet{drawil2013gps} demonstrating the GPS fixes
%         produced by a receiver in a vehicle as the vehicle drove through a built
%         up area. Grey lines indicate roads, whilst coloured dots indicate GPS
%         fixes, coloured according to the output of an automated precision
%         classifier.}
%         \label{fig:drawil-et-al}
%     \end{figure}

    Fortunately, road vehicles in motion are almost always situated on public
    roads, so it is possible to improve the accuracy of a localisation system by
    taking into account the information about the public road network which is
    present in a digital street map. With the advent of the OpenStreetMap
    project\footnotemark---which provides, free of charge, detailed road network
    maps covering most of the globe---use of map information in this way is
    quickly becoming a cheap and convenient method of augmenting the accuracy of
    existing localisation systems.

    \footnotetext{\url{http://www.openstreetmap.org/}}

    The most common method of incorporating this information is through ``map
    matching'', where localisation is performed under the simplifying assumption
    that the true location will \emph{always} coincide with a roadway, footpath
    or other mapped feature. In this paper, we will not make such a highly
    restrictive assumption. Instead, we will merely assume that the vehicle is
    \emph{most likely} near a roadway, and attempt to use this assumption to
    improve localisation accuracy as greatly as we can without losing the
    ability to localise vehicles which stray from the road network.

    \section{Related work}

    The approaches which presently dominate the map matching literature
    typically use sets of hand-crafted rules to determine which road segment in
    a digital map a vehicle is most likely to be travelling on, then attempt to
    localise the vehicle within that road segment, typically by localising to
    the point on the segment nearest to the last GPS fix or Kalman filter state
    estimate \citep{quddus2007current}. These ``heuristic'' methods have a
    number of % TODO: Talk about shortcomings

    % TODO: Explain heuristic algorithms, then segue into probabilistic
    % algorithms.

    % Also need to talk, in particular, about Selloum et al.'s, article, as well
    % as Dellaert's article and the MCL papers.

    \section{Particle filtering}

    \section{The localisation algorithm}\label{sec:algorithm}

    \subsection{Incorporating map data}
    % Efficiently incorporating map data
    Computing the distance between each particle and its nearest road segment is
    by far the most expensive step in the particle filtering process, so it is
    imperative that this computation be fast if real-time performance is
    desired. We have found that the k-d tree implementation provided by
    \citet{alliez20153d} yields good performance for this task, taking around
    50ms to perform the calculations for 2000 particles on a map of 3220
    segments using a 2.5GHz Intel Core i5 processor.

    \subsection{GPS dropouts}

    \subsection{Unavailability of odometry}

    \section{Results}

    In order to evaluate the performance the map-based localisation algorithm
    proposed in Section~\ref{sec:algorithm}, we utilised both quantitative and
    qualitative assessment. In the quantitative tests, we attempted to analyse
    the performance of the algorithm on partially artificial data for which it
    was straightforward to measure divergence from the ground truth. On the
    other hand, the qualitative tests attempted to examine algorithm performance
    using more realistic consumer-grade localisation data, which unfortunately
    did not include a ground truth for use in numerical comparisons.

    The quantitative tests utilised the data set presented by
    \citet{brubaker2013lost}, which includes a number of GPS traces along with
    OpenStreetMap data for the regions immediately surrounding the traces. The
    supplied dataset included exceptionally accurate RTK-GPS positioning data,
    but no corresponding samples with a more representative noise model. As a
    result, we had to use a synthetic noise model to benchmark the algorithm,
    which included the following ``roughening'' steps:

    \begin{enumerate}
        % TODO
        \item
    \end{enumerate}

    The qualitative assessment used a GPS trace kindly provided by J. Alvarez
    (personal communication, May 12, 2015), which was produced from several
    kilometers of urban driving, including a brief GPS outage corresponding to a
    tunnel. The results of this test are presented in Figure % TODO

    % IDEA: Apparently along-track error is much lower than cross-track error in
    % built-up areas because of the fact that buildings, which cause multipath
    % interference, are located on either side of the road. This makes the fact
    % that my algorithm reduces along-track error quite helpful.

    \section{Conclusion}

    \bibliography{citations}{}
    \bibliographystyle{abbrvnat}
\end{document}
