\documentclass[11pt,twocolumn]{article}
% Note: Steve doesn't use super or sort&compress (although AFAICT he does
% compress). Uses a mixture of citet and cite in-text. Having said that, he
% doesn't express a strong preference; some papers use all citep with no numeric
% citations!
\usepackage[super,square,comma,sort&compress]{natbib}
% Uncomment the following to get inline links to references.
\usepackage{hyperref}
\usepackage{todonotes}
\usepackage{tabularx}
\usepackage[hmargin=0.5in,vmargin=0.8in]{geometry}

% Big collection of notes and TODOs, since I don't want to insert these all
% throughout the text:
% - First paragraph of the introduction is needlessly long-winded, as is the
%   abstract. May need to delete these entirely, as Steve said that we don't
%   need a foreword if it's explaining things that are already obvious to our
%   audience (e.g. a review on GPU parallel processing for a general CS audience
%   doesn't need an explanation of what parallel processing is, but might
%   benefit from a brief explanation on how GPUs parallelise work).
% - Apparently we can cite surveys and reviews to reduce the length of our
%   bibliography. We just have to include the magic words "see <THING> and
%   references therein".
% - Bibliography isn't counted towards page limit, and page limit is just a
%   guideline, but Steve DOES value conciseness and WILL penalise you if you
%   write over the word limit without reason.
% - It's good to mix and match existing research paths and speculate on what
%   people can do in the future.
% - In the task sheet, we're explicitly told "Your report should not summarise
%   each paper individually. Rather it should distil the main messages and
%   commonalities into a form that would be useful to a researcher approaching
%   the field for the first time. Think about what you would have liked to have
%   read when you started on this research topic." I've actually just summarised
%   some papers, and I should rewrite the offending sections so that they're not
%   paper-specific.
% - Need to summarise performance measures and write up a comparison of MM
%   performance in a table.
% - Should be more critical of different methods so that the reader understands
%   what kind of challenges they're likely to face. Actual papers aren't very
%   helpful here, so I'll really have infer a lot of these weaknesses and read
%   between the lines in subsequent papers for others. Here are a couple of
%   things I haven't mentioned yet which are common problems:
%    - There are no common data sets or performance measures for assessing map
%    matching algorithms. This makes it really difficult to compare performance,
%    and impossible to reproduce reported results.
%    - Most algorithms either fail to solve the kidnapped robot problem or solve
%    it with an unacceptable performance penalty (resorting to initial matching,
%    which could take half a minute or more).
% - Might help to re-categorise the algorithms I've been considering into just
%   "heuristic" (uses some simple measures which correlate positively or
%   negatively with appropriate segments, and combines these using simple
%   weightings and/or basic algorithms) and "probabilistic" (approaches which
%   start from a probability distribution over segments or positions and attempt
%   to maximise either the posterior over positions or the likelihood over
%   segments or something similar).
% - Need to use MM as an acronym for map matching. Will save half a page, at the
%   least :P

\begin{document}
    \title{A Survey of Map Matching Algorithms\\
    {\Large COMP2550 Literature Survey}}
    \author{Sam Toyer\\\texttt{u5568237@anu.edu.au}}
    \date{\today}

    \maketitle

    \abstract{
        Map matching is the process of taking a series of observations of a
        vehicle's environment---for instance, Global Positioning System (GPS)
        measurements of the vehicle's latitude and longitude, or the distance
        travelled by the vehicle's rear wheels in a fixed time period---and
        attempting to fit the path described by those observations onto a
        digital map. In this survey, we explore the current state-of-the-art in
        online map matching algorithms, and evaluate the reported efficacy and
        applicability of these algorithms to the problem of map matching on
        low-detail maps.
    }

    \section{Introduction}
        Whilst contemporary satellite-based navigation systems like GPS can be
        used to estimate the coordinates of a vehicle just about anywhere on the
        surface of the earth, the inaccuracy inherent in these systems can still
        be significant for some applications, especially in built-up areas. For
        objects which are assumed to be on roadways, these errors can be reduced
        by considering the information present in freely available digital
        street maps like those produced by the OpenStreetMap\footnotemark
        project. The task of an online map matching algorithm is to take noisy
        or biased observations from a localisation or odometry system and
        attempt to find the most accurate projection of the object being tracked
        onto a given digital map.

        \footnotetext{\url{http://www.openstreetmap.org/}}

        \missingfigure{Illustration of how a road network is represented in a
        digital map, and what a map matched route looks like when projected onto
        that network.}

        For the purposes of this survey, we will divide map matching algorithms
        up into three categories: firstly, we will consider basic heuristic
        algorithms which employ simple geometric and topological features of the
        road network. Next, we will look at more sophisticated algorithms which
        take advantage of several such heuristics, either by using a simple
        weighting or through more complex methods like fuzzy logic. Finally, we
        will consider probabilistic map matching algorithms.

    \section{Heuristic algorithms}
        % Cover point-to-point, point-to-curve, curve-to-curve, and possibly
        % some other methods. Just really basic turn-of-the-century stuff.
        Initial solutions to the map matching problem used simple geometric
        the road segments making up a digital map. Later algorithms also took
        into account the topology of the road graph and the location history of
        the vehicle to try and match the ``shape'' of the vehicle's path to the
        map. Whilst these approaches offer mediocre performance on their
        own\cite{quddus2007current}, we nonetheless consider some of them in
        this section, as they form the basis for many of the more advanced
        algorithms which constitute the state-of-the-art today.

        \citet{white2000some} offer an excellent summary of several such basic
        algorithms, and we will briefly summarise two of them here.

        The first algorithm from \citet{white2000some} which we will consider,
        dubbed \textit{point-to-curve matching}, simply finds the nearest road
        segment in a digital map and calculates the orthogonal projection of the
        most recent GPS measurement onto that segment. As the authors point out,
        this algorithm not only fails to make use of historical data, vehicle
        heading, vehicle velocity, topological information and so on, but also
        results in a highly unstable output at intersections, where the reported
        GPS reading may be approximately equidistant to two or more road
        segments.

        The second algorithm which we will consider, \textit{curve-to-curve
        matching}, attempts to incorporate both historical position data and map
        topology. It begins by constructing a piecewise linear curve from the
        $n$ most recent GPS measurements for the vehicle. It then locates nodes
        in the road graph within a certain distance of the most recent GPS
        measurement. For each of these nodes, it constructs additional piecewise
        linear curves of the same length as that produced from the vehicle's
        positioning history, each of which follow the road network outwards from
        the node in question. Finally, it considers the area between each of the
        curves on the map and the curve representing the vehicle's path, and
        selects the curve with the lowest such area. The output of the algorithm
        is a projection of the vehicle's current GPS position onto the best
        curve.

        % XXX: Either here or later I should contrast these methods with methods
        % which use heading data and other information, since inclusion of
        % heading data is such a massive boon.

        Simple matching methods like point-to-curve and curve-to-curve can be
        combined using a number of schemes to produce improved map matching
        algorithms. These more advanced algorithms typically follow a three-step
        process:

        \begin{enumerate}
            \item Firstly, the algorithm estimates the vehicle's global
            position. This estimate can come directly from a GPS sensor, but is
            often combined with odometry data and passed through an Extended
            Kalman Filter (EKF) in order to improve accuracy. This step is
            independent of the choice of map matching algorithm, so we will
            treat the precise positioning method used as a black box for the
            remainder of this survey.
            \item \label{itm:seg} Next, the algorithm attempts to find the
            specific road segment which the vehicle is travelling on.
            \item \label{itm:intra} Finally, the algorithm produces a best guess
            as to where the vehicle is within the road segment. This step is
            often as simple as a point-to-curve projection for the road segment
            from (\ref{itm:seg}).
        \end{enumerate}

        Step \ref{itm:seg} is generally the most complex part of the matching
        process, as an incorrectly identified road segment will lead to a poor
        estimate of position. This step is often split into several different
        ``modes'', depending on the state of the vehicle. For instance, if it is
        not known which road segment the vehicle was travelling on in the
        previous time segment, then the algorithm may take precautions---like
        waiting until reported GPS error is below a threshold---in order to
        ensure that the first matched segment is correct. If the first segment
        is not correct, then it is likely that subsequently identified segments
        will not be correct either, as map matching algorithms often choose not
        to consider segments which are not neighbours of the previously matched
        segment. A similar issue occurs at junctions, where the vehicle can turn
        onto one of several different road segments, and the consequences for
        incorrectly inferring which segment the vehicle has branched onto are
        similar to those for incorrectly inferring the initial segment.

        % TODO: Write down which papers take this approach, and then say
        % "...since this is so simple, we won't consider it for the reminder of
        % this section."
        On the other hand, step \ref{itm:intra} is generally simple, and usually
        consists of a simple projection of the last GPS fix onto the matched
        road segment. As such, we will not consider it for the remainder of this
        section so that we can focus on the more complex link selection process.

        % Things I'll mention here:
        %  - One or two examples of manually weighted heuristic approaches.
        %  Probably Ochieng et. al. and the paper Jose put on the project page.
        %  Possibly use Velaga et. al.'s 2009 paper instead of Ochieng's since
        %  that's more recent.
        %  - One or two examples of DS theory
        %  - One or two examples of fuzzy logic

        % Current papers:
        %  - Maybe Ochieng et al. if I can, since they got encouraging results.
        %  Ugh, not sure though. Greenfeld would be perfect here.
        %  - Velaga et al. (2009) is a good choice for illustrating the
        %  potential (and difficulty!) of learning parameters in weighted
        %  algorithms.
        % - Not sure whether I should include more recent work from Quddus and
        %   co. It's really just re-hashing old ideas /over and over and over/
        %   at this point.
        \todo[inline]{Weighting algorithms. Ochieng et. al. is a good choice, as
        is \citet{velaga2009developing}}

        Fuzzy logic is another commonly used method of incorporating multiple
        heuristics for map matching. Fuzzy logic systems consider a variety of
        numerical inputs---for instance, distance to a road segment $d$ or speed
        of a vehicle $s$---and \textit{fuzzify} these according to
        \textit{membership functions} which map a numerical input like velocity
        to a degree of applicability of some particular classification, like
        ``fast'' or ``slow''. It can then apply a number of simple inference
        rules like the following, which attempt to determine how likely it is
        that a vehicle is on a given road segment $r_i$:

        \begin{center}
            \setlength{\tabcolsep}{2pt}
            \begin{tabular}{llllll}
                If & $d$ is short & and & $s$ is fast & then & $r_i$ is unlikely\\
                If & $d$ is long & and & $s$ is medium & then & $r_i$ is likely
            \end{tabular}
        \end{center}

        Each of the above rules will use some predefined method to find the
        degree to which $d$ is ``short'' or ``long'' and the degree to which $s$
        is ``medium'' or ``fast'', then combine the weights for these labels
        together and use them to find the degree to which $r_i$ is ``likely'' or
        ``unlikely''. These degree to which each label is applicable is then
        combined back into a real number using one of several method, and this
        number is taken to represent how likely it is that the vehicle is on
        segment $r_i$.

        The precise way in which input variables are mapped onto labels like
        ``fast'' and ``slow'', as well as the way in which these labels are
        combined to produce a numerical result, varies from application to
        application. However, the common quality of fuzzy logic MM systems is
        that they allow the designer to incorporate their own domain knowledge
        into MM system in a natural way using inference rules like those given
        above.

        % Should probably comment on the fact that Quddus et al.'s work is
        % probably not reproducible because of the complexity of the algorithm.
        \citet{quddus2006high} propose a highly effective, fuzzy-logic based
        method which merges odometry, heading information, connectivity, and GPS
        fixes smoothed using an EKF. Their link detection mechanism operates in
        one of three modes: one for determining the initial link when no
        map-matched positions are available, one for determining subsequent
        links when travelling between junctions, and one for determining
        subsequent links at junctions. Each of these modes uses a separate set
        of fuzzy inference rules, which take into account GPS precision as
        reported by the receiver, the vehicle's speed, heading and position
        relative to the link, distance and direction to the nearest junction,
        and the vehicle's change in heading over time. The authors report a
        correct link identification rate of 99.2\% using this algorithm, as
        noted in Table~\ref{tab:comparison}.

        Fuzzy logic algorithms have many of the same flaws as the weighted
        heuristic ones mentioned at the beginning of this section; like weighted
        algorithms, fuzzy logic algorithms have a large number of parameters and
        design decisions which can be difficult to optimise, and are often
        determined through trial and error. Fuzzy logic algorithms also require
        a high degree of domain knowledge and experimentation to write inference
        rules, which are not required in simple weighted models. Nevertheless,
        fuzzy logic has yielded excellent results in map matching.

    \section{Probabilistic algorithms}
        Probabilistic algorithms approach map matching as a problem of trying to
        find a probability distribution over all possible positions of a vehicle
        on the road network, given the available sensor data. This distribution
        can be approximated using a number of techniques, and the resultant
        approximation used to provide a reasonable estimate as to the location
        of the vehicle on the map, as well as a measure of certainty for this
        estimate.

        The most common probabilistic technique for map matching is particle
        filtering. A particle filter approximates a vehicle's position on the
        road using a fixed number of ``particles'' at different locations, each
        representing a different possible location for the vehicle. Each
        particle has an associated weight, which intuitively indicates how well
        that particle corresponds to the set of sensory inputs to the system.
        Initially, the particles are scattered uniformly throughout some region,
        and all weights are equal. At each time step, the particle filter does
        the following, as described at length in \citet{pf2002}:

        \begin{enumerate}
            \item Each particle's weight is updated using the likelihood of the
            most recently acquired sensor data, under the hypothesis that the
            true location of the vehicle is that of the given particle.
            \item A new set of $N$ particles is then sampled, with replacement,
            from the old set of $N$ particles according to the particles'
            weights. The weights of the particles are then set to be uniform.
            \item Finally, the state of each particle is advanced according to a
            model of the vehicle's motion. Some noise may be added during this
            process to ensure that subsequent re-sampling steps do not eliminate
            all but the most probable particle state.
        \end{enumerate}

        Particle filters are employed in a similar manner by
        \citet{toledo2009fusing} and \citet{selloum2009lane}. The algorithms
        presented in each of these papers incorporate map data by forcing a
        particle's weight to zero whenever its distance to the nearest road
        centerline exceeds a half a standard lane width, ensuring that they will
        be eliminated at the next resampling step. The algorithms also
        incorporate data from onboard gyroscopes and odometers in order to
        advance update the state of each particle, and can use GPS data when it
        is available, which is a boon in environments where GPS signals are
        patchy.

        One common disadvantage to these approaches is that they rely on the
        availability of ``enhanced maps'' (or ``Emaps''), which use clothoids
        (also known as Euler or Cornu spirals) to represent road segments, and
        are thus able to more accurately express the path of curved roads. The
        algorithms also rely on information about lane widths, although this can
        be estimated from national road standards if necessary. Another drawback
        is that particle filters can cease to function properly when a large
        number of particles have their weights clamped to zero during an update
        step, as happens when a particle leaves the road, and neither paper
        addresses this weakness in detail.

        Another more recent approach to probabilistic map matching, which still
        requires knowledge of lane location and direction, but dispenses with
        the requirement for lane width, is given by \citet{brubaker2013lost}.
        Rather than approximating the vehicle's location distribution using
        particles, \citet{brubaker2013lost} use a mixture of weighted Gaussian
        distributions assigned to each road segment. At each time step, these
        Gaussians are updated using both simulated vehicle dynamics and visual
        odometry. The vehicle's location can then be inferred from the mode of
        the position distribution.

        Despite not using GPS data, in one test this algorithm was reported to
        be able to go from a uniform distribution over 2000km of roads to a
        positioning estimate within 3m of ground truth after only 20s of
        observations. The chief weakness of this approach is that it sometimes
        fails to localise the vehicle when there exist multiple possible routes
        on a map with approximately the same shape, as such routes are not
        distinguishable using odometry alone.

    \section{Evaluation of existing results}
        \begin{table*}[t]
            \begin{tabularx}{\textwidth}{|X|X|X|X|X|X|X|}
                \hline
                Paper & Sensors & Family & Identification \% & ATE & XTE &
                HE\\
                \hline
                \citet{quddus2006high} & GPS, gyro, XXX & Fuzzy logic & XX.XX\%
                & XXm & XXm & XXm\\
                \hline
            \end{tabularx}

            \caption{
                Performance data for map matching algorithms mentioned in
                    this survey. In some cases, multiple measures of performance data are
                    available, in which case each measure is followed by a reference to
                    the paper in which that measure appeared. Columns list, in
                    order: paper in which the algorithm appeared, sensors
                    required by the algorithm, algorithm family, correct segment
                    identification percentage, along-track error (ATE), cross-track
                    error (XTE) and horizontal error (HE).
            }

            \label{tab:comparison}
        \end{table*}

        At present, the only established and consistently available metric for
        evaluating map matching algorithms is the proportion of road segments
        which are correctly identified. Other metrics like along-track error and
        cross-track error are seldom available, and even when they are, the data
        and code used to produce them is not readily available, making it
        difficult to compare algorithms presented in different papers.

        Nevertheless, the available performance data for the papers mentioned in
        this survey, mostly self-reported is summarised in
        Table~\ref{tab:comparison}.

    \section{Conclusion}
        \todo[inline]{Summarise results}

    \bibliography{citations}{}
    \bibliographystyle{abbrvnat}
\end{document}
