\documentclass[11pt]{article}

\usepackage[round]{natbib}
\usepackage{hyperref}
\usepackage{xcolor}
\hypersetup{
    % See
    % http://tex.stackexchange.com/questions/823/remove-ugly-borders-around-clickable-cross-references-and-hyperlinks
    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
}
\usepackage{todonotes}
\usepackage{tabularx}
\usepackage{multirow}
\usepackage{multicol}
\usepackage[hmargin=0.5in,vmargin=0.8in]{geometry}

% See http://tex.stackexchange.com/questions/2441/how-to-add-a-forced-line-break-inside-a-table-cell
\newcommand{\vcell}[2][c]{\begin{tabular}[1]{@{}l@{}}#2\end{tabular}}
\newcommand{\sdag}{\textsuperscript{\textdagger}}
% Big collection of notes and TODOs, since I don't want to insert these all
% throughout the text:
% - First paragraph of the introduction is needlessly long-winded, as is the
%   abstract. May need to delete these entirely, as Steve said that we don't
%   need a foreword if it's explaining things that are already obvious to our
%   audience (e.g. a review on GPU parallel processing for a general CS audience
%   doesn't need an explanation of what parallel processing is, but might
%   benefit from a brief explanation on how GPUs parallelise work).
% - Apparently we can cite surveys and reviews to reduce the length of our
%   bibliography. We just have to include the magic words "see <THING> and
%   references therein".
% - Bibliography isn't counted towards page limit, and page limit is just a
%   guideline, but Steve DOES value conciseness and WILL penalise you if you
%   write over the word limit without reason.
% - It's good to mix and match existing research paths and speculate on what
%   people can do in the future.
% - In the task sheet, we're explicitly told "Your report should not summarise
%   each paper individually. Rather it should distil the main messages and
%   commonalities into a form that would be useful to a researcher approaching
%   the field for the first time. Think about what you would have liked to have
%   read when you started on this research topic." I've actually just summarised
%   some papers, and I should rewrite the offending sections so that they're not
%   paper-specific.
% - Need to summarise performance measures and write up a comparison of MM
%   performance in a table.
% - Should be more critical of different methods so that the reader understands
%   what kind of challenges they're likely to face. Actual papers aren't very
%   helpful here, so I'll really have infer a lot of these weaknesses and read
%   between the lines in subsequent papers for others. Here are a couple of
%   things I haven't mentioned yet which are common problems:
%    - There are no common data sets or performance measures for assessing map
%    matching algorithms. This makes it really difficult to compare performance,
%    and impossible to reproduce reported results.
%    - Most algorithms either fail to solve the kidnapped robot problem or solve
%    it with an unacceptable performance penalty (resorting to initial matching,
%    which could take half a minute or more).
% - Might help to re-categorise the algorithms I've been considering into just
%   "heuristic" (uses some simple measures which correlate positively or
%   negatively with appropriate segments, and combines these using simple
%   weightings and/or basic algorithms) and "probabilistic" (approaches which
%   start from a probability distribution over segments or positions and attempt
%   to maximise either the posterior over positions or the likelihood over
%   segments or something similar).
% - Need to use MM as an acronym for map matching. Will save half a page, at the
%   least :P
% - Using "Lane-Level Integrity Provision for Navigation and Map Matching With GNSS, Dead
%   Reckoning, and Enhanced Maps" instead of the other Toledo-Moreo et al. paper
%   might be more charitable.

\begin{document}
    \title{A Survey of Map Matching Algorithms\\
    {\Large COMP2550 Literature Survey}}
    \author{Sam Toyer\\\texttt{u5568237@anu.edu.au}}
    \date{\today}

    \maketitle

    \abstract{
        Map matching is the process of taking a series of observations of a
        vehicle's environment---for instance, Global Positioning System (GPS)
        measurements of the vehicle's latitude and longitude, or the distance
        travelled by the vehicle's rear wheels in a fixed time period---and
        attempting to fit the path described by those observations onto a
        digital map. In this survey, we explore the current state-of-the-art in
        online map matching algorithms, and evaluate the reported efficacy and
        applicability of these algorithms to the problem of map matching on
        low-detail maps.
    }

\begin{multicols}{2}
    \section{Introduction}
        Whilst contemporary satellite-based navigation systems like GPS can be
        used to estimate the coordinates of a vehicle just about anywhere on the
        surface of the earth, the inaccuracy inherent in these systems can still
        be significant for some applications, especially in built-up areas. For
        objects which are assumed to be on roadways, these errors can be reduced
        by considering the information present in freely available digital
        street maps like those produced by the OpenStreetMap\footnotemark
        project. The task of an online map matching algorithm is to take noisy
        or biased observations from a localisation or odometry system and
        attempt to find the most accurate projection of the object being tracked
        onto a given digital map.

        \footnotetext{\url{http://www.openstreetmap.org/}}

        For the purposes of this survey, we will divide map matching algorithms
        up into three categories: firstly, we will consider basic heuristic
        algorithms which employ simple geometric and topological features of the
        road network. Next, we will look at more sophisticated algorithms which
        take advantage of several such heuristics, either by using a simple
        weighting or through more complex methods like fuzzy logic. Finally, we
        will consider probabilistic map matching algorithms.

    \section{Heuristic algorithms}
        % Cover point-to-point, point-to-curve, curve-to-curve, and possibly
        % some other methods. Just really basic turn-of-the-century stuff.
        Initial solutions to the map matching problem used simple geometric the
        road segments making up a digital map. Later algorithms also took into
        account the topology of the road graph and the location history of the
        vehicle to try and match the ``shape'' of the vehicle's path to the map.
        Whilst these approaches offer middling performance on their own
        \citep{quddus2007current}, we nonetheless consider some of them in this
        section, as they form the basis for many of the more advanced algorithms
        which constitute the state-of-the-art today.

        \citet{white2000some} offer an excellent summary of several such basic
        algorithms, and we will briefly summarise two of them here.

        The first algorithm from \citeauthor{white2000some} which we will consider,
        dubbed \textit{point-to-curve matching}, simply finds the nearest road
        segment in a digital map and calculates the orthogonal projection of the
        most recent GPS measurement onto that segment. As the authors point out,
        this algorithm not only fails to make use of historical data, vehicle
        heading, vehicle velocity, topological information and so on, but also
        results in a highly unstable output at intersections, where the reported
        GPS reading may be approximately equidistant to two or more road
        segments.

        The second algorithm which we will consider, \textit{curve-to-curve
        matching}, attempts to incorporate both historical position data and map
        topology. It begins by constructing a piecewise linear curve from the
        $n$ most recent GPS measurements for the vehicle. It then locates nodes
        in the road graph within a certain distance of the most recent GPS
        measurement. For each of these nodes, it constructs additional piecewise
        linear curves of the same length as that produced from the vehicle's
        positioning history, each of which follow the road network outwards from
        the node in question. Finally, it considers the area between each of the
        curves on the map and the curve representing the vehicle's path, and
        selects the curve with the lowest such area. The output of the algorithm
        is a projection of the vehicle's current GPS position onto the best
        curve.

        % XXX: Either here or later I should contrast these methods with methods
        % which use heading data and other information, since inclusion of
        % heading data is such a massive boon.

        Simple matching methods like point-to-curve and curve-to-curve can be
        combined using a number of schemes to produce improved map matching
        algorithms. These more advanced algorithms typically follow a three-step
        process:

        \begin{enumerate}
            \item Firstly, the algorithm estimates the vehicle's global
            position. This estimate can come directly from a GPS sensor, but is
            often combined with odometry data and passed through an Extended
            Kalman Filter (EKF) in order to improve accuracy. This step is
            independent of the choice of map matching algorithm, so we will
            treat the precise positioning method used as a black box for the
            remainder of this survey.
            \item \label{itm:seg} Next, the algorithm attempts to find the
            specific road segment which the vehicle is travelling on.
            \item \label{itm:intra} Finally, the algorithm produces a best guess
            as to where the vehicle is within the road segment. This step is
            often as simple as a point-to-curve projection for the road segment
            from (\ref{itm:seg}).
        \end{enumerate}

        Step \ref{itm:seg} is generally the most complex part of the matching
        process, as an incorrectly identified road segment will lead to a poor
        estimate of position. This step is often split into several different
        ``modes'', depending on the state of the vehicle. For instance, if it is
        not known which road segment the vehicle was travelling on in the
        previous time segment, then the algorithm may take precautions---like
        waiting until reported GPS error is below a threshold---in order to
        ensure that the first matched segment is correct. If the first segment
        is not correct, then it is likely that subsequently identified segments
        will not be correct either, as map matching algorithms often choose not
        to consider segments which are not neighbours of the previously matched
        segment. A similar issue occurs at junctions, where the vehicle can turn
        onto one of several different road segments, and the consequences for
        incorrectly inferring which segment the vehicle has branched onto are
        similar to those for incorrectly inferring the initial segment.

        % XXX: Actually, might want to consider algorithms which use, say, a
        % per-segment Kalman filter or some other more advanced mechanism (e.g.
        % Fouque, Bonnifait & Betaille '08). Only cover if I have space (looking
        % unlikely at this point).
        On the other hand, step \ref{itm:intra} is generally simple, and usually
        consists of a simple projection of the last GPS fix onto the matched
        road segment. As such, we will not consider it for the remainder of this
        section so that we can focus on the more complex link selection process.

        \citet{ochieng2003map} propose an algorithm which is typical of more
        advanced map matching procedures. Upon initialisation, it selects an
        initial link based primarily on link distance from the vehicle's
        Kalman-filtered position estimate. It then enters into a subsequent
        matching phase, during which it either projects GPS coordinates onto a
        previously selected link. If the vehicle begins to turn or nears a
        junction, the algorithm re-enters its initial link selection phase to
        detect which road segment the vehicle has entered.

        One of the drawbacks to this approach is that it involves a lot of
        ``magic numbers''; for instance, heading difference thresholds are
        required to determine when the vehicle is turning, and distance
        thresholds are required to determine when the vehicle is near a
        junction. \citeauthor{ochieng2003map} tweak these constants manually,
        but this method is labor-intensive, and may need to be repeated for
        different environments, like rural or suburban environments, in which
        different thresholds may be advantageous.

        \citet{velaga2009developing} present a similar algorithm, albeit one
        which uses a linear combination of normalised features related to
        vehicle heading, link heading, distance along link and link connectivity
        to select new links when the vehicle turns or reaches a junction. As a
        result of their use of weighting for link features, their algorithm is
        even more dependent on tunable constants than that of
        \citeauthor{ochieng2003map}, and \citeauthor{velaga2009developing}
        address this issue by proposing an accompanying optimisation procedure
        for finding weights.

        Optimising weights in a highly stateful algorithm is challenging, as the
        most useful error function for a given set of weights---the proportion
        of mismatched roads---can generally only be computed by performing a
        complete run of the map-matching algorithm over the available data.
        \citeauthor{velaga2009developing} handle this by assuming that map
        matching error can be approximated by a simple function of the weights
        used by their algorithm. Once this approximation to the error function
        is found using an optimisation algorithm,
        \citeauthor{velaga2009developing} choose the weights used by their map
        matching algorithm to be the set of weights which minimise this
        learnt approximation.

        \citet{velaga2012improving} achieve a small increase in performance of
        this algorithm through two augmentations. Firstly, they adopt a genetic
        algorithm for optimising their weights, and re-tune some of the manually
        derived thresholds used by their algorithm to match their larger data
        set. Secondly, they use junction density in the proximity of the vehicle
        to determine whether the vehicle is in a rural, suburban or urban
        environment, and choose a different set of weights for their algorithm
        based on this observation. The change in weights and thresholds yields
        minimal improvement on their previous algorithm, but produces an
        improvement in link identification accuracy of around two percentage
        points when combined with their environment-specific weight selection
        approach, as stated in Table~\ref{tab:comparison}.

        Fuzzy logic is another commonly used method of incorporating multiple
        heuristics for map matching. Fuzzy logic systems consider a variety of
        numerical inputs---for instance, distance to a road segment $d$ or speed
        of a vehicle $s$---and ``fuzzify'' these according to membership
        functions which map a numerical input like velocity to a degree of
        applicability of some particular classification, like ``fast'' or
        ``slow''. It can then apply a number of simple inference rules like the
        following, which attempt to determine how likely it is that a vehicle is
        on a given road segment $r_i$:

        \begin{center}
            \setlength{\tabcolsep}{2pt}
            \begin{tabular}{llllll}
                If & $d$ is short & and & $s$ is fast & then & $r_i$ is unlikely\\
                If & $d$ is long & and & $s$ is medium & then & $r_i$ is likely
            \end{tabular}
        \end{center}

        Each of the above rules will use some predefined method to find the
        degree to which $d$ is ``short'' or ``long'' and the degree to which $s$
        is ``medium'' or ``fast'', then combine the weights for these labels
        together and use them to find the degree to which $r_i$ is ``likely'' or
        ``unlikely''. These degree to which each label is applicable is then
        combined back into a real number using one of several method, and this
        number is taken to represent how likely it is that the vehicle is on
        segment $r_i$.

        The precise way in which input variables are mapped onto labels like
        ``fast'' and ``slow'', as well as the way in which these labels are
        combined to produce a numerical result, varies from application to
        application. However, the common quality of fuzzy logic MM systems is
        that they allow the designer to incorporate their own domain knowledge
        into MM system in a natural way using inference rules like those given
        above.

        % Should probably comment on the fact that Quddus et al.'s work is
        % probably not reproducible because of the complexity of the algorithm.
        \citet{quddus2006high} propose a highly effective, fuzzy-logic based
        method which merges odometry, heading information, connectivity, and GPS
        fixes smoothed using an EKF. Their link detection mechanism operates in
        one of three modes: one for determining the initial link when no
        map-matched positions are available, one for determining subsequent
        links when travelling between junctions, and one for determining
        subsequent links at junctions. Each of these modes uses a separate set
        of fuzzy inference rules, which take into account GPS precision as
        reported by the receiver, the vehicle's speed, heading and position
        relative to the link, distance and direction to the nearest junction,
        and the vehicle's change in heading over time. The authors report a
        correct link identification rate of 99.2\% using this algorithm, as
        noted in Table~\ref{tab:comparison}.

        Fuzzy logic algorithms have many of the same flaws as the weighted
        heuristic ones mentioned at the beginning of this section; like weighted
        algorithms, fuzzy logic algorithms have a large number of parameters and
        design decisions which can be difficult to optimise, and are often
        determined through trial and error. Fuzzy logic algorithms also require
        a high degree of domain knowledge and experimentation to write inference
        rules, which are not required in simple weighted models. Nevertheless,
        fuzzy logic has yielded excellent results in map matching.

    \section{Probabilistic algorithms}
        Probabilistic algorithms approach map matching as a problem of trying to
        find a probability distribution over all possible positions of a vehicle
        on the road network, given the available sensor data. This distribution
        can be approximated using a number of techniques, and the resultant
        approximation used to provide a reasonable estimate as to the location
        of the vehicle on the map, as well as a measure of certainty for this
        estimate.

        The most common probabilistic technique for map matching is particle
        filtering. A particle filter approximates a vehicle's position on the
        road using a fixed number of ``particles'' at different locations, each
        representing a different possible location for the vehicle. Each
        particle has an associated weight, which intuitively indicates how well
        that particle corresponds to the set of sensory inputs to the system.
        Initially, the particles are scattered uniformly throughout some region,
        and all weights are equal. At each time step, the particle filter does
        the following, as described at length by \citet{pf2002}:

        \begin{enumerate}
            \item Each particle's weight is updated using the likelihood of the
            most recently acquired sensor data, under the hypothesis that the
            true location of the vehicle is that of the given particle.
            \item A new set of $N$ particles is then sampled, with replacement,
            from the old set of $N$ particles according to the particles'
            weights. The weights of the particles are then set to be uniform.
            \item Finally, the state of each particle is advanced according to a
            model of the vehicle's motion. Some noise may be added during this
            process to ensure that subsequent re-sampling steps do not eliminate
            all but the most probable particle state.
        \end{enumerate}

        Particle filters are employed in a similar manner by
        \citet{toledo2009fusing} and \citet{selloum2009lane}. The algorithms
        presented in each of these papers incorporate map data by forcing a
        particle's weight to zero whenever its distance to the nearest road
        centerline exceeds a half a standard lane width, ensuring that they will
        be eliminated at the next resampling step. The algorithms also
        incorporate data from onboard gyroscopes and odometers in order to
        advance update the state of each particle, and can use GPS data when it
        is available, which is a boon in environments where GPS signals are
        patchy.

        One common disadvantage to these approaches is that they rely on the
        availability of ``enhanced maps'' (or ``Emaps''), which use clothoids
        (also known as Euler or Cornu spirals) to represent road segments, and
        are thus able to more accurately express the path of curved roads. The
        algorithms also rely on information about lane widths, although this can
        be estimated from national road standards if necessary. Another drawback
        is that particle filters can cease to function properly when a large
        number of particles have their weights clamped to zero during an update
        step, as happens when a particle leaves the road, and neither paper
        addresses this weakness in detail.

        Another more recent approach to probabilistic map matching, which still
        requires knowledge of lane location and direction, but dispenses with
        the requirement for lane width, is given by \citet{brubaker2013lost}.
        Rather than approximating the vehicle's location distribution using
        particles, \citeauthor{brubaker2013lost} use a mixture of weighted
        Gaussian distributions assigned to each road segment. At each time step,
        these Gaussians are updated using both simulated vehicle dynamics and
        visual odometry. The vehicle's location can then be inferred from the
        mode of the position distribution.

        Despite only requiring odometry data, which can be derived either using
        computer vision methods or from GPS fixes, this algorithm was reported
        to be able to go from a uniform distribution over 2000km of roads to a
        positioning estimate within 3m of ground truth after only 20s of
        observations. The chief weakness of this approach is that it sometimes
        fails to localise the vehicle when there exist multiple possible routes
        on a map with approximately the same shape, as such routes are not
        distinguishable using odometry alone.

    \section{Examination of existing results and future research directions}
        At present, the only established and consistently available metric for
        evaluating map matching algorithms is the proportion of road segments
        which are correctly identified. Other metrics like along-track error and
        cross-track error are seldom available, and even when they are, the data
        and code used to produce them is not readily available, making it
        difficult to compare algorithms presented in different papers.
        Furthermore, some benchmarks are performed on extremely limited
        datasets, and should be interpreted within the context provided in the
        their respective papers. In spite of these caveats, the available
        performance data for the algorithms mentioned in this survey is
        summarised in Table~\ref{tab:comparison} of the appendix.

        Future research in map matching could benefit greatly from standardised
        map matching benchmarking methods. At the very least, reporting link
        matching accuracy and horizontal positioning error would assist other
        researchers in making meaningful comparisons between algorithms, as
        would the use of publicly available data sets like that produced by
        \citet{newson2009hidden}.\footnotemark

        \footnotetext
        {\url{http://research.microsoft.com/en-us/um/people/jckrumm/MapMatchingData/data.htm}}

        \todo[inline]{Something}

        % Things I want to say:
        % - There has not been enough research into making probabilistic
        %   algorithms perform well with low-quality street data.
        % - Further, it would be interesting to see the effect of certain
        %   pre-processing steps on map matching accuracy. Brubaker et al. run a
        %   cubic interpolation process on their map data before running it
        %   through their algorithm, and IIRC some other researchers do
        %   something similar. However, it's not clear how this affects map
        %   matching accuracy, if at all. Other assumptions, like lane width,
        %   would also be interesting to evaluate empirically.
        % - So, a summary of the first two points would be: we now have OSM, so
        %   we should be looking at ways to use oodles of relatively simple map
        %   data effectively instead of relying on highly specialised clothoid
        %   models or "secret sauce" maps obtained from commercial sources.
        % - Hybrid heuristic algorithms currently appear to  give the best
        %   performance, but are difficult to tune and are highly sensitive to
        %   initial positioning fixes. How could this be addressed?
        % - The fact that high performance heuristic algorithms /all/ use some
        %   sort of modal process for matching at junctions and on links is
        %   suspicious. Is there any reason why we can't get away with one mode?
        %   Doing so would greatly simplify optimisation, do away with initial
        %   link dependence /and/ fix the problem of subsequent matches becoming
        %   wonky after one mismatch. I suspect the answer to this question is
        %   "no, you can't, because real-time map matching will require a run
        %   from beginning to start since future predictions are reliant on past
        %   ones". However, it doesn't make sense that this should be the case;
        %   why can't we just make our process reliant on past fixes rather than
        %   past matches? After all, relying on past matches rather than fixes
        %   is really just throwing out oodles of information. Something like
        %   Newson & Krum's HMM would be interesting in this regard However, it
        %   doesn't make sense that this should be the case; why can't we just
        %   make our process reliant on past fixes rather than past matches?
        %   After all, relying on past matches rather than fixes is really just
        %   throwing out oodles of information. Something like a sliding window
        %   version of Newson & Krum's HMM would be interesting in this regard.
        % - So, maybe my conclusion should be something like "Heuristic
        %   approaches yield the best performance, but more research is needed
        %   into automatically tuning these approaches to yield the best
        %   possible performance".
        % - I want to see a fuzzy model with some sort of automatic tuning.
        %   Wasn't there a field for neural fuzzy models or something like that?
        % - Best plausible reported results are for a fuzzy logic model, but I
        %   can't find anybody else who has achieved the same results using
        %   fuzzy logic. This suggests that fuzzy logic algorithms are quite
        %   difficult to produce, and may not have a significant advantage over
        %   other topological methods.

    \section{Conclusion}
        \todo[inline]{Summarise results}

    \bibliography{citations}{}
    \bibliographystyle{abbrvnat}
\end{multicols}

    \clearpage
    \appendix
    \section*{Appendix: Algorithm performance data}
        \begin{table}[h]
            {\small
            \begin{tabularx}{\textwidth}{X|X|X|l|l|l|l}
                Paper & Extra information & Algorithm type & Identification \% &
                ATE & XTE & HE\\
                \hline

                \multirow{2}{2.5cm}{\citet{white2000some}} &
                \multirow{2}{2.5cm}{GPS} &
                Point-to-curve &
                \vcell{53.4--67.7\%\\
                76.8\%\sdag} &
                29.5m (95\%)\sdag &
                10.1m (95\%)\sdag &
                32.0m (95\%)\sdag\\

                & & Curve-to-curve &
                61.7--72.6\% &
                - &
                - &
                -\\

                \citet{pf2002} &
                Wheel speeds, lane data &
                Particle filter &
                - &
                - &
                - &
                -\\

                \citet{ochieng2003map} &
                GPS, gyro, odometer &
                Heuristic &
                100\% &
                - &
                - &
                -\\

                \citet{quddus2006high} &
                GPS, gyro, odometer &
                Fuzzy logic &
                99.2\% &
                4.2m (95\%) &
                3.2m (95\%) &
                5.5m (95\%)\\

                \citet{toledo2009fusing} &
                GPS, lane data, gyro, odometer &
                Particle filter &
                - &
                - &
                - &
                1.9m (95\%)\\

                \citet{selloum2009lane} &
                GPS, lane data, gyro, odometer &
                Particle filter &
                73\% &
                - &
                - &
                -\\

                \citet{velaga2009developing} &
                GPS, gyro &
                Heuristic &
                95.9--96.7\% &
                7.36m (95\%) &
                9m (95\%) &
                9.8m (95\%)\\

                % TODO
                \citet{velaga2012improving} &
                GPS, gyro &
                Heuristic &
                97.8\% &
                2.11m$\pm$1.52m &
                3.19m$\pm$2.59m &
                4.19m$\pm$2.47m\\

                \citet{brubaker2013lost} &
                GPS odometry, lane data &
                Gaussian mixture model &
                - &
                - &
                - &
                2.4m (mean)\\
            \end{tabularx}}

            \caption{
                Performance data for map matching algorithms mentioned in this
                survey, as reported by their respective authors except where
                indicated otherwise. Figures marked with a superscript
                dagger\sdag\ are taken from \citet{quddus2006high}. Column
                contents are, from left to right: paper in which the algorithm
                appeared, sensors required by the algorithm, algorithm family,
                correct segment identification percentage, along-track error
                (ATE), cross-track error (XTE) and horizontal error relative to
                ground truth from an augmented GPS system (HE). Positioning
                errors are either given as percentiles, means and standard
                deviations (mean$\pm$standard deviation), or just means, as
                indicated.
            }

            \label{tab:comparison}
        \end{table}
\end{document}
