\documentclass[11pt,twocolumn]{article}
% Note: Steve doesn't use super or sort&compress (although AFAICT he does
% compress). Uses a mixture of citet and cite in-text. Having said that, he
% doesn't express a strong preference; some papers use all citep with no numeric
% citations!
\usepackage[super,square,comma,sort&compress]{natbib}
% Uncomment the following to get inline links to references.
\usepackage{hyperref}
\usepackage{todonotes}
\usepackage{tabularx}
\usepackage[hmargin=0.5in,vmargin=0.8in]{geometry}

% Big collection of notes and TODOs, since I don't want to insert these all
% throughout the text:
%  - First paragraph of the introduction is needlessly long-winded, as is the
%  abstract.
%  - In the task sheet, we're explicitly told "Your report should not summarise
%  each paper individually. Rather it should distil the main messages and
%  commonalities into a form that would be useful to a researcher approaching
%  the field for the first time. Think about what you would have liked to have
%  read when you started on this research topic." I've actually just summarised
%  some papers, and I should rewrite the offending sections so that they're not
%  paper-specific.
%  - Need to summarise performance measures and write up a comparison of
%  MM performance in a table.
%  - Should be more critical of different methods so that the reader understands
%  what kind of challenges they're likely to face. Actual papers aren't very
%  helpful here, so I'll really have infer a lot of these weaknesses and read
%  between the lines in subsequent papers for others.

\begin{document}
    \title{A Survey of Map Matching Algorithms\\
    {\Large COMP2550 Literature Survey}}
    \author{Sam Toyer\\\texttt{u5568237@anu.edu.au}}
    \date{\today}

    \maketitle

    \abstract{
        Map matching is the process of taking a series of observations of a
        vehicle's environment---for instance, Global Positioning System (GPS)
        measurements of the vehicle's latitude and longitude, or the distance
        travelled by the vehicle's rear wheels in a fixed time period---and
        attempting to fit the path described by those observations onto a
        digital map. In this survey, we explore the current state-of-the-art in
        online map matching algorithms, and evaluate the reported efficacy and
        applicability of these algorithms to the problem of map matching on
        low-detail maps.
    }

    \section{Introduction}
        Whilst contemporary satellite-based navigation systems like GPS can be
        used to estimate the coordinates of a vehicle just about anywhere on the
        surface of the earth, the inaccuracy inherent in these systems can still
        be significant for some applications, especially in built-up areas:
        \citet{modsching2006field} report mean errors of up to 15m in some urban
        environments, for instance. For objects which are assumed to be on
        roadways, these errors can be dramatically reduced by considering the
        information present in freely available digital street maps like those
        produced by the OpenStreetMap\footnotemark project. The task of an
        online map matching algorithm is to take noisy or biased observations
        from a localisation or odometry system and attempt to find the most
        accurate projection of the object being tracked onto a given digital
        map.

        \footnotetext{\url{http://www.openstreetmap.org/}}

        \missingfigure{Illustration of how a road network is represented in a
        digital map, and what a map matched route looks like when projected onto
        that network.}

        For the purposes of this survey, we will divide map matching algorithms
        up into three categories: firstly, we will consider basic heuristic
        algorithms which employ simple geometric and topological features of the
        road network. Next, we will look at more sophisticated algorithms which
        take advantage of several such heuristics, either by using a simple
        weighting or through more complex methods like Dempster-Shafer theory
        and fuzzy logic. Finally, we will consider more advanced probabilistic
        algorithms, and give particular consideration to those which are capable
        of estimating the possible positions of a vehicle on several road
        segments at a time, or of localising the vehicle at the lane-level.

    \section{Basic heuristic algorithms}
        % Cover point-to-point, point-to-curve, curve-to-curve, and possibly
        % some other methods. Just really basic turn-of-the-century stuff.
        Initial solutions to the map matching problem used simple geometric
        the road segments making up a digital map. Later algorithms also took
        into account the topology of the road graph and the location history of
        the vehicle to try and match the ``shape'' of the vehicle's path to the
        map. Whilst these approaches offer mediocre performance on their
        own\cite{quddus2007current}, we nonetheless consider some of them in
        this section, as they form the basis for many of the more advanced
        algorithms which constitute the state-of-the-art today.

        \citet{white2000some} offer an excellent summary of several such basic
        algorithms, and we will briefly summarise two of them here.

        The first algorithm \citet{white2000some} consider, dubbed
        \textit{point-to-curve matching}, simply finds the nearest road segment
        in a digital map and calculates the orthogonal projection of the most
        recent GPS measurement onto that segment. As the authors point out, this
        algorithm not only fails to make use of historical data, vehicle
        heading, vehicle velocity, topological information and so on, but also
        results in a highly unstable output at intersections, where the reported
        GPS reading may be approximately equidistant to two or more road
        segments.

        As a result of the deficiencies in this algorithm, the authors propose
        two augmentations of it. The first considers the difference between the
        vehicle's heading and the alignment of the given street segments, and
        discards segments with too great a difference between the vehicle's
        heading and the segment's orientation. The second defines ``nearby''
        roads by their driving distance along the map from the current estimated
        vehicle position, rather than by their straight-line distance from the
        most recent GPS reading.\todo[inline]{Offer criticism of this approach,
        or possible delete it since it will not be useful later.}

        The final algorithm \citet{white2000some} consider,
        \textit{curve-to-curve matching}, attempts to incorporate both
        historical position data and map topology. It begins by constructing a
        piecewise linear curve from the $n$ most recent GPS measurements for the
        vehicle. It then locates nodes in the road graph within a certain
        distance of the most recent GPS measurement. For each of these nodes, it
        constructs additional piecewise linear curves of the same length as that
        produced from the vehicle's positioning history, each of which follow
        the road network outwards from the node in question. Finally, it
        considers the area between each of the curves on the map and
        the curve representing the vehicle's path, and selects the curve with
        the lowest such area. The output of the algorithm is a projection of the
        vehicle's current GPS position onto the best curve.

    \section{Combining multiple heuristics}
        % This section: algorithms using a weighting of heuristics, or merging
        % heuristics together using DS theory or fuzzy logic (there are *many*
        % such localisation systems)
        Simple heuristics like those mentioned in the previous section can be
        combined using a number of schemes to produce improved map-matching
        algorithms. These more advanced algorithms typically follow a three-step
        process:

        \begin{enumerate}
            \item Firstly, the algorithm estimates the vehicle's global
            position. This estimate can come directly from a GPS sensor, but is
            often combined with odometry data and passed through an Extended
            Kalman Filter (EKF) in order to improve accuracy. This step is
            independent of the choice of map matching algorithm, so we will
            treat the precise positioning method used as a black box for the
            remainder of this survey.
            \item Next, the algorithm attempts to find the specific road
            segment which the vehicle is travelling on.
            \item Finally, the algorithm produces a best guess as to where the
            vehicle is within the road segment.
        \end{enumerate}

        % Things I'll mention here:
        %  - One or two examples of manually weighted heuristic approaches.
        %  Probably Ochieng et. al. and the paper Jose put on the project page.
        %  Possibly use Velaga et. al.'s 2009 paper instead of Ochieng's since
        %  that's more recent.
        %  - One or two examples of DS theory
        %  - One or two examples of fuzzy logic
        \todo[inline]{Weighting algorithms. Ochieng et. al. is a good choice, as
        is \citet{velaga2009developing}}

        Fuzzy logic is another commonly used method of incorporating multiple
        heuristics for map matching. Fuzzy logic systems consider a variety of
        numerical inputs---for instance, distance to a road segment $d$ or speed
        of a vehicle $s$---and \textit{fuzzify} these according to
        \textit{membership functions} which map a numerical input like velocity
        to a degree of applicability of some particular classification, like
        ``fast'' or ``slow''. It can then apply a number of simple inference
        rules like the following, which attempt to determine how likely it is
        that a vehicle is on a given road segment $r_i$:

        \begin{center}
            \setlength{\tabcolsep}{2pt}
            \begin{tabular}{llllll}
                If & $d$ is short & and & $s$ is fast & then & $r_i$ is unlikely\\
                If & $d$ is long & and & $s$ is medium & then & $r_i$ is likely
            \end{tabular}
        \end{center}

        Each of the above rules will use some predefined method to find the
        degree to which $d$ is ``short'' or ``long'' and the degree to which $s$
        is ``medium'' or ``fast'', then combine the weights for these labels
        together and use them to find the degree to which $r_i$ is ``likely'' or
        ``unlikely''. These degree to which each label is applicable is then
        combined back into a real number using one of several method, and this
        number is taken to represent how likely it is that the vehicle is on
        segment $r_i$.

        The precise way in which input variables are mapped onto labels like
        ``fast'' and ``slow'', as well as the way in which these labels are
        combined to produce a numerical result, varies from application to
        application. However, the common quality of fuzzy logic MM systems is
        that they allow the designer to incorporate their own domain knowledge
        into MM system in a natural way using inference rules like those given
        above.

        % XXX: Need to define EKFs beforehand
        \citet{quddus2006high} propose a highly effective, fuzzy-logic based
        method which merges odometry, heading information, connectivity, and GPS
        fixes smoothed using an EKF. Their link detection mechanism operates in
        one of three modes: one for determining the initial link when no
        map-matched positions are available, one for determining subsequent
        links when travelling between junctions, and one for determining
        subsequent links at junctions. Each of these modes uses a separate set
        of fuzzy inference rules, which take into account GPS precision as
        reported by the receiver, the vehicle's speed, heading and position
        relative to the link, distance and direction to the nearest junction,
        and the vehicle's change in heading over time. The authors report a
        correct link identification rate of 99.2\% using this algorithm, as
        noted in Table~\ref{tab:comparison}.

        \citet{quddus2006high} also utilise a slightly more sophisticated method
        for determining the location of a vehicle within a specific link. First,
        they perform point-to-curve matching to produce one estimate of the
        vehicle's position. Next, they use vehicle's previously estimated
        position on the link, its speed and the heading of the link to provide
        another position estimate. These two estimates are then merged to
        produce a final estimate by weighting them according to their
        approximated variance.

        Fuzzy logic algorithms have many of the same flaws as the weighted
        heuristic ones mentioned at the beginning of this section; like weighted
        algorithms, fuzzy logic algorithms have a large number of parameters and
        design decisions which can be difficult to optimise, and are often
        determined through trial and error. Fuzzy logic algorithms also require
        a high degree of domain knowledge and experimentation to write inference
        rules, which are not required in simple weighted models. Nevertheless,
        fuzzy logic has yielded excellent results in map matching.

    \section{Probabilistic methods}
        % I especially want to write about multi-hypothesis methods, where we're not
        % entirely sure what segment we're on, so we maintain a probability
        % distribution over segments (or a segment-independent probability
        % distribution over vehicle states), as opposed to the traditional method
        % where we select a promising segment and localise on it.

        % Things I'll mention here:
        %  - A couple of particle filter approaches. I feel that the EMap work
        %    is particularly relevant here because it shows what can be done
        %    with more accurate maps. First approach I'll cover will be PF
        %  - The GMM approach from Lost!
        \todo[inline]{Particle filtering}

        \todo[inline]{GMM approach from Lost!}

    \section{Quantitative evaluation of results}
        \begin{table*}[t]
            \begin{tabularx}{\textwidth}{|X|X|X|}
                \hline
                Foo & Bar & Baz\\
                \hline
                Something & else & here\\
                Something & other & here\\
                \hline
            \end{tabularx}
        \end{table*}

    \section{Conclusion}
        \todo[inline]{Summarise results}

    \bibliography{citations}{}
    \bibliographystyle{abbrvnat}
\end{document}
